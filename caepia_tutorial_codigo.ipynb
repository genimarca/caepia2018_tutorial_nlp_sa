{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTVbXvqsI2-F"
   },
   "source": [
    "# Cabecera\n",
    "* Importación de librerías\n",
    "* Definición de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iZkoHKkbJPoI",
    "outputId": "50036a46-03f6-4a98-9995-5d942f5c9b2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy.random import rand as np_rand\n",
    "from numpy.random import seed as np_seed\n",
    "from numpy import array as np_array\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.svm.classes import LinearSVC\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUgbBfpiJS-Z"
   },
   "outputs": [],
   "source": [
    "TWEET_TOKENIZER = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "CLASSES = []\n",
    "EMB_SEP_CHAR = \" \"\n",
    "RE_TOKEN_USER = re.compile(r\"(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){20}(?!@))|(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){1,19})(?![A-Za-z0-9_]*@)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqobKswAI4Jn"
   },
   "source": [
    "# Lectura de Datos\n",
    "* Lectura corpus\n",
    "* Lectura embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwRPCvuZRGE-"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    '''Load the corpus into memory\n",
    "    '''\n",
    "    \n",
    "    ids = []\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    ids_append = ids.append\n",
    "    classes_append = CLASSES.append\n",
    "    labels_append = labels.append\n",
    "    tweets_append = tweets.append\n",
    "    with(open(path, 'r', encoding='utf-8')) as input_file:\n",
    "        own_split = str.split\n",
    "        own_strip = str.strip\n",
    "        input_file.readline()\n",
    "        for buffer in input_file:\n",
    "            buffer_fields = own_split(buffer, ';;;')\n",
    "            ids_append(own_strip(buffer_fields[0]))\n",
    "            label = own_strip(buffer_fields[4])\n",
    "            if(label not in CLASSES):\n",
    "                classes_append(label)\n",
    "            labels_append(CLASSES.index(label))\n",
    "            tweets_append(own_strip(buffer_fields[-1]))\n",
    "    \n",
    "    return(ids, labels, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cckyuMPVRct-"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(path, offset):\n",
    "    \"\"\"Load embeddings file.\n",
    "    \"\"\"\n",
    "    word_embeddings = [[] for i in range(offset)]\n",
    "    word_indexes = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as emb_file:\n",
    "        emb_file.readline()\n",
    "        for line in emb_file:\n",
    "            fields = line.partition(EMB_SEP_CHAR)\n",
    "            word = fields[0].strip()\n",
    "            own_strip = str.strip\n",
    "            emb_values = np_array([float(x) for x in own_strip(fields[-1]).split(EMB_SEP_CHAR)])\n",
    "            word_indexes[word] = len(word_embeddings)\n",
    "            word_embeddings.append(emb_values)\n",
    "\n",
    "    return (word_embeddings, word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBOTRiwOSo04"
   },
   "source": [
    "# Utilidades\n",
    "* Función para tokenizar.\n",
    "* Función para generar entrada RNN\n",
    "* Función para generar entrada RNN con embeddings pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3E417tbYTbjP"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize an input text\n",
    "    \n",
    "    Args:\n",
    "        text: A String with the text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        A list of Strings (tokens)\n",
    "    \"\"\"\n",
    "    text_tokenized = TWEET_TOKENIZER.tokenize(text)\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeEP60haTjte"
   },
   "outputs": [],
   "source": [
    "def fit_transform_vocabulary(corpus):\n",
    "    \"\"\"Creates the vocabulary of the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list os str (documents)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple whose first element is a dictionary word-index and the second\n",
    "        element is a list of list in which each position is the index of the \n",
    "        token in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = {}\n",
    "    corpus_indexes = []\n",
    "    corpus_indexes_append = corpus_indexes.append\n",
    "    index = 2\n",
    "    for doc in corpus:\n",
    "        doc_indexes = []\n",
    "        tokens = tokenize(doc)\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = index\n",
    "                doc_indexes.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                doc_indexes.append(vocabulary[token])\n",
    "        \n",
    "        corpus_indexes_append(doc_indexes)\n",
    "        \n",
    "    return (vocabulary, corpus_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW25G4Z4Tnlm"
   },
   "outputs": [],
   "source": [
    "def fit_transform_vocabulary_pretrain_embeddings(corpus, pre_embeddings_index):\n",
    "    \"\"\"Creates the vocabulary of the corpus.\n",
    "        Index 0: padding\n",
    "        Index 1: OOV.\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list os str (documents)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple whose first element is a dictionary word-index and the second\n",
    "        element is a list of list in which each position is the index of the \n",
    "        token in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = {}\n",
    "    corpus_indexes = []\n",
    "    corpus_indexes_append = corpus_indexes.append\n",
    "    index = 0\n",
    "    own_lowercase = str.lower\n",
    "    for doc in corpus:\n",
    "        doc_indexes = []\n",
    "        tokens = tokenize(own_lowercase(doc))\n",
    "        for token in tokens:\n",
    "            if RE_TOKEN_USER.fullmatch(token):\n",
    "                token = \"@user\"\n",
    "            if token in pre_embeddings_index:\n",
    "                index = pre_embeddings_index[token]\n",
    "                doc_indexes.append(index)\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = index\n",
    "            else:\n",
    "                index = 1\n",
    "                doc_indexes.append(index)\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = index\n",
    "        corpus_indexes_append(doc_indexes)\n",
    "        \n",
    "    return (vocabulary, corpus_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSw4mkXWTtuv"
   },
   "source": [
    "# Clasificadores\n",
    "\n",
    "\n",
    "*   SVM con TF-IDF\n",
    "*   RNN-LSTM con TF-IDF\n",
    "*   RNN-LSTM con embeddings aleatorios\n",
    "*   RNN-LSTM con embeddings pre-entrenados (FastText)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN4BHYLcUKzU"
   },
   "outputs": [],
   "source": [
    "def classification_linear_svm(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classifies using SVM as classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    \n",
    "    classifier = LinearSVC(multi_class=\"ovr\", random_state=random_state)\n",
    "    print(\"Start SVM training\")\n",
    "    classifier = classifier.fit(train_sparse_matrix_features_tfidf, labels_train)\n",
    "    print(\"Finish SVM training\")\n",
    "    y_labels = classifier.predict(test_sparse_matrix_features_tfidf)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9r-rCv1Vqvb"
   },
   "outputs": [],
   "source": [
    "def classification_tfidf_rnn(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification using a RNN with tfidf as features\n",
    "    \"\"\"\n",
    "    np_seed(random_state)\n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    train_features_tfidf = []\n",
    "    own_train_features_tfidf_append = train_features_tfidf.append\n",
    "    lengths_tweets = []\n",
    "    own_lengths_tweets_append = lengths_tweets.append\n",
    "    \n",
    "    for tweet in train_sparse_matrix_features_tfidf:\n",
    "        own_train_features_tfidf_append(tweet.data)\n",
    "        own_lengths_tweets_append(len(tweet.data))\n",
    "    \n",
    "\n",
    "    test_features_tfidf = [tweet.data for tweet in test_sparse_matrix_features_tfidf]\n",
    "    #Average length\n",
    "    max_len_input = int(np.average(lengths_tweets, 0))\n",
    "    #lstm_output_dim = int(2**np.log2(max_len_input))\n",
    "    \n",
    "    #NN model\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(LSTM(64, input_shape=(max_len_input,1)))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[0]))\n",
    "    print(\"Training features:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[-1]))\n",
    "    print(\"Training parameters:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[0]*train_sparse_matrix_features_tfidf.shape[-1]))\n",
    "    print(nn_model.summary())\n",
    "\n",
    "    train_features_tfidf_pad = sequence.pad_sequences(train_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=train_sparse_matrix_features_tfidf.dtype)\n",
    "    train_features_tfidf_pad = np.expand_dims(train_features_tfidf_pad, axis=-1)\n",
    "    print(\"Start RNN LSTM training\")\n",
    "    nn_model.fit(train_features_tfidf_pad, labels_train, batch_size=32, epochs=10, verbose=1)\n",
    "    print(\"Finish RNN LSTM training\")\n",
    "    test_features_tfidf_pad = sequence.pad_sequences(test_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=test_sparse_matrix_features_tfidf.dtype)\n",
    "    test_features_tfidf_pad = np.expand_dims(test_features_tfidf_pad, axis=-1)\n",
    "    y_labels = nn_model.predict_classes(test_features_tfidf_pad, batch_size=32, verbose=1)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJoyKyT5Ykad"
   },
   "outputs": [],
   "source": [
    "def classifcation_embedings_rnn(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification with RNN and embedings (no pre-trained)\n",
    "    \"\"\"\n",
    "    np_seed(random_state)\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "\n",
    "    #Build vocabulary and corpus indexes\n",
    "    vocabulary_train, corpus_train_index = fit_transform_vocabulary(tweets_train)\n",
    "    \n",
    "    \n",
    "    max_len_input = int(np.average([len(tweet_train) for tweet_train in corpus_train_index], 0))\n",
    "    \n",
    "    corpus_test_index = []\n",
    "    own_corpus_test_index_append = corpus_test_index.append\n",
    "    for tweet_test in tweets_test:\n",
    "        tokens_test = tokenize(tweet_test)\n",
    "        own_corpus_test_index_append([vocabulary_train.get(token_test, 1) for token_test in tokens_test])\n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Embedding(len(vocabulary_train)+2, 100, input_length=max_len_input, trainable=False))\n",
    "    nn_model.add(LSTM(64))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(len(tweets_train)))\n",
    "    print(\"Training features (vocabulary):\\t{}\".format(len(vocabulary_train)))\n",
    "    print(\"Training doc x features:\\t{}\".format(len(tweets_train)*len(vocabulary_train)))\n",
    "    print(\"Training vocabulary embeddings:\\t{}\".format(len(vocabulary_train)*100))\n",
    "    print(\"Training parameters:\\t{}\".format(len(tweets_train)*len(vocabulary_train)*100))\n",
    "    print(nn_model.summary())\n",
    "    \n",
    "\n",
    "    train_features_pad = sequence.pad_sequences(corpus_train_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train_index[0][0]))\n",
    "    \n",
    "    \n",
    "    print(\"Start RNN EMBEDDING LSTM training\")\n",
    "    nn_model.fit(train_features_pad, labels_train, batch_size=32, epochs=25, verbose=1)\n",
    "    print(\"Finish RNN EMBEDDING LSTM training\")\n",
    "    test_features_pad = sequence.pad_sequences(corpus_test_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_test_index[0][0]))\n",
    "    y_labels = nn_model.predict_classes(test_features_pad, batch_size=32, verbose=1)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxwKgQoJYuP3"
   },
   "outputs": [],
   "source": [
    "def classifcation_pretrain_embedings_rnn(tweets, train_index, test_index, labels_train, embeddings_path, random_state=None):\n",
    "    \"\"\"Classification with RNN and embedings (no pre-trained)\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "\n",
    "    #Offset = 2; Padding and OOV.\n",
    "    print(\"Begin loading embeddings.\")\n",
    "    word_embeddings, word_emb_indexes = read_embeddings(embeddings_path, 2)\n",
    "    print(\"End loading embeddings.\")\n",
    "    np_seed(random_state)\n",
    "    word_embeddings[0] = 2 * 0.1 * np_rand(len(word_embeddings[2])) - 1\n",
    "    word_embeddings[1] = 2 * 0.1 * np_rand(len(word_embeddings[2])) - 1\n",
    "\n",
    "    #Build vocabulary and corpus indexes\n",
    "    vocabulary_train, corpus_train_index = fit_transform_vocabulary_pretrain_embeddings(tweets_train, word_emb_indexes)\n",
    "    \n",
    "    \n",
    "    max_len_input = int(np.average([len(tweet_train) for tweet_train in corpus_train_index], 0))\n",
    "    \n",
    "    corpus_test_index = []\n",
    "    own_corpus_test_index_append = corpus_test_index.append\n",
    "    own_lowercase = str.lower\n",
    "    for tweet_test in tweets_test:\n",
    "        tokens_test = tokenize(own_lowercase(tweet_test))\n",
    "        doc_test_index = []\n",
    "        for token_test in tokens_test:\n",
    "            if RE_TOKEN_USER.fullmatch(token_test) is not None:\n",
    "                token_test = \"@user\"\n",
    "            doc_test_index.append(word_emb_indexes.get(token_test, 1))\n",
    "        own_corpus_test_index_append(doc_test_index)\n",
    "    \n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Embedding(len(word_embeddings), len(word_embeddings[0]), weights=[np_array(word_embeddings)], input_length=max_len_input, trainable=False))\n",
    "    nn_model.add(LSTM(64))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(len(tweets_train)))\n",
    "    print(\"Training features (vocabulary/embeddings):\\t{}\".format(len(word_embeddings)))\n",
    "    print(\"Training doc x features:\\t{}\".format(len(tweets_train)*len(word_embeddings)))\n",
    "    print(\"Training vocabulary embeddings:\\t{}\".format(len(word_embeddings)*100))\n",
    "    print(\"Training input features:\\t{}\".format(len(tweets_train)*max_len_input))\n",
    "    print(\"Training input features:\\t{}\".format(len(tweets_train)*max_len_input*100))\n",
    "    print(\"Training parameters:\\t{}\".format(len(tweets_train)*len(word_embeddings)*100))\n",
    "    print(nn_model.summary())\n",
    "    \n",
    "\n",
    "    train_features_pad = sequence.pad_sequences(corpus_train_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train_index[0][0]))\n",
    "    \n",
    "    \n",
    "    print(\"Start RNN PRETRAIN EMBEDDING LSTM training\")\n",
    "    nn_model.fit(train_features_pad, labels_train, batch_size=32, epochs=25, verbose=1)\n",
    "    print(\"Finish RNN PRETRAIN EMBEDDING LSTM training\")\n",
    "    test_features_pad = sequence.pad_sequences(corpus_test_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_test_index[0][0]))\n",
    "    y_labels = nn_model.predict_classes(test_features_pad, batch_size=32, verbose=1)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DITTgeFzYwy5"
   },
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMSuCFRzYy-e"
   },
   "outputs": [],
   "source": [
    "def calculate_quality_performamnce(y_labels, y_classified_labels, model_name):\n",
    "    \n",
    "    classes_index = [CLASSES.index(c) for c in CLASSES]\n",
    "    accruacy = metrics.accuracy_score(y_labels, y_classified_labels)\n",
    "    macro_precision = metrics.precision_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_recall = metrics.recall_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_f1 = metrics.f1_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    \n",
    "    print(\"\\n*** Results \" + model_name + \"***\")\n",
    "    print(\"Macro-Precision: \" + str(macro_precision))\n",
    "    print(\"Macro-Recall: \" + str(macro_recall))\n",
    "    print(\"Macro-F1: \" + str(macro_f1))\n",
    "    print(\"Accuracy: \" + str(accruacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfVqkjSKY3XA"
   },
   "source": [
    "# Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-175WWrzY9oC"
   },
   "source": [
    "Configuración semilla aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFWooq_rY5PX"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJMeepvfZFCY"
   },
   "outputs": [],
   "source": [
    "input_file_path=\"tass14_general_corpus_train.csv\"\n",
    "input_embeddings_path=\"fasttext_spanish_twitter_100d.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bta9PLPTZioe"
   },
   "source": [
    "Leer corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atNMjlXSZfOa"
   },
   "outputs": [],
   "source": [
    "ids, labels, tweets = read_corpus(input_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMpx1ZQZZmC2"
   },
   "source": [
    "Partición entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYZ-7SloZoM8"
   },
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(np.arange(len(tweets)), test_size=0.2, random_state=7)\n",
    "labels_train = [labels[tweet_index] for tweet_index in train_index]\n",
    "labels_test = [labels[tweet_index] for tweet_index in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJNpKmVeZs_N"
   },
   "source": [
    "Ejecución clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yx_C9gAeZwk-",
    "outputId": "c67d2250-7ffa-41a9-9ea2-17b97b06218a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start SVM training\n",
      "Finish SVM training\n"
     ]
    }
   ],
   "source": [
    "y_labels_svn = classification_linear_svm(tweets, train_index, test_index, labels_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "Jcph9Np0cUtp",
    "outputId": "b5cce772-ea59-4606-86d3-60aedc30eaa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Summary of the model\n",
      "Training samples:\t5774\n",
      "Training features:\t18799\n",
      "Training parameters:\t108545426\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                16896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 19,108\n",
      "Trainable params: 19,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Start RNN LSTM training\n",
      "Epoch 1/10\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.2522 - acc: 0.4067\n",
      "Epoch 2/10\n",
      "5774/5774 [==============================] - 5s 781us/step - loss: 1.2244 - acc: 0.4112\n",
      "Epoch 3/10\n",
      "5774/5774 [==============================] - 5s 858us/step - loss: 1.2157 - acc: 0.4134\n",
      "Epoch 4/10\n",
      "5774/5774 [==============================] - 5s 890us/step - loss: 1.2145 - acc: 0.4117\n",
      "Epoch 5/10\n",
      "5774/5774 [==============================] - 7s 1ms/step - loss: 1.2125 - acc: 0.4181\n",
      "Epoch 6/10\n",
      "5774/5774 [==============================] - 4s 621us/step - loss: 1.2105 - acc: 0.4240\n",
      "Epoch 7/10\n",
      "5774/5774 [==============================] - 4s 619us/step - loss: 1.2093 - acc: 0.4205\n",
      "Epoch 8/10\n",
      "5774/5774 [==============================] - 4s 702us/step - loss: 1.2097 - acc: 0.4233\n",
      "Epoch 9/10\n",
      "5774/5774 [==============================] - 3s 597us/step - loss: 1.2062 - acc: 0.4250\n",
      "Epoch 10/10\n",
      "5774/5774 [==============================] - 4s 668us/step - loss: 1.2078 - acc: 0.4181\n",
      "Finish RNN LSTM training\n",
      "1444/1444 [==============================] - 0s 204us/step\n"
     ]
    }
   ],
   "source": [
    "y_labels_rnn = classification_tfidf_rnn(tweets, train_index, test_index, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "id": "CZD-fL7hcZDA",
    "outputId": "525ff7c1-133c-4a6a-e58c-220d9d2a0ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the model\n",
      "Training samples:\t5774\n",
      "Training features (vocabulary):\t18799\n",
      "Training doc x features:\t108545426\n",
      "Training vocabulary embeddings:\t1879900\n",
      "Training parameters:\t10854542600\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 19, 100)           1880100   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,924,552\n",
      "Trainable params: 44,452\n",
      "Non-trainable params: 1,880,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Start RNN EMBEDDING LSTM training\n",
      "Epoch 1/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.2234 - acc: 0.4224\n",
      "Epoch 2/25\n",
      "5774/5774 [==============================] - 5s 838us/step - loss: 1.1996 - acc: 0.4311\n",
      "Epoch 3/25\n",
      "5774/5774 [==============================] - 5s 882us/step - loss: 1.1746 - acc: 0.4654\n",
      "Epoch 4/25\n",
      "5774/5774 [==============================] - 6s 989us/step - loss: 1.1631 - acc: 0.4782\n",
      "Epoch 5/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.1480 - acc: 0.4887\n",
      "Epoch 6/25\n",
      "5774/5774 [==============================] - 6s 975us/step - loss: 1.1458 - acc: 0.4946\n",
      "Epoch 7/25\n",
      "5774/5774 [==============================] - 5s 807us/step - loss: 1.1400 - acc: 0.5007\n",
      "Epoch 8/25\n",
      "5774/5774 [==============================] - 5s 827us/step - loss: 1.1331 - acc: 0.5028\n",
      "Epoch 9/25\n",
      "5774/5774 [==============================] - 5s 806us/step - loss: 1.1329 - acc: 0.4986\n",
      "Epoch 10/25\n",
      "5774/5774 [==============================] - 5s 898us/step - loss: 1.1292 - acc: 0.5045\n",
      "Epoch 11/25\n",
      "5774/5774 [==============================] - 6s 959us/step - loss: 1.1237 - acc: 0.5062\n",
      "Epoch 12/25\n",
      "5774/5774 [==============================] - 5s 903us/step - loss: 1.1189 - acc: 0.5107\n",
      "Epoch 13/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.1167 - acc: 0.5100\n",
      "Epoch 14/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.1112 - acc: 0.5166\n",
      "Epoch 15/25\n",
      "5774/5774 [==============================] - 5s 950us/step - loss: 1.1071 - acc: 0.5125\n",
      "Epoch 16/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.1036 - acc: 0.5194\n",
      "Epoch 17/25\n",
      "5774/5774 [==============================] - 7s 1ms/step - loss: 1.1023 - acc: 0.5163\n",
      "Epoch 18/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.0938 - acc: 0.5230\n",
      "Epoch 19/25\n",
      "5774/5774 [==============================] - 5s 920us/step - loss: 1.0907 - acc: 0.5227\n",
      "Epoch 20/25\n",
      "5774/5774 [==============================] - 5s 937us/step - loss: 1.0902 - acc: 0.5260\n",
      "Epoch 21/25\n",
      "5774/5774 [==============================] - 6s 954us/step - loss: 1.0812 - acc: 0.5263\n",
      "Epoch 22/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.0789 - acc: 0.5367\n",
      "Epoch 23/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.0725 - acc: 0.5374\n",
      "Epoch 24/25\n",
      "5774/5774 [==============================] - 7s 1ms/step - loss: 1.0654 - acc: 0.5461\n",
      "Epoch 25/25\n",
      "5774/5774 [==============================] - ETA: 0s - loss: 1.0613 - acc: 0.543 - 6s 1ms/step - loss: 1.0607 - acc: 0.5445\n",
      "Finish RNN EMBEDDING LSTM training\n",
      "1444/1444 [==============================] - 0s 331us/step\n"
     ]
    }
   ],
   "source": [
    "y_labels_embeddings_rnn = classifcation_embedings_rnn(tweets, train_index, test_index, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1360
    },
    "colab_type": "code",
    "id": "URAG5MRbcaON",
    "outputId": "cf02d958-f792-46f1-eaa0-79b463c5c846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading embeddings.\n",
      "End loading embeddings.\n",
      "Summary of the model\n",
      "Training samples:\t5774\n",
      "Training features (vocabulary/embeddings):\t513342\n",
      "Training doc x features:\t2964036708\n",
      "Training vocabulary embeddings:\t51334200\n",
      "Training input features:\t109706\n",
      "Training input features:\t10970600\n",
      "Training parameters:\t296403670800\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 100)           51334200  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 51,378,652\n",
      "Trainable params: 44,452\n",
      "Non-trainable params: 51,334,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Start RNN PRETRAIN EMBEDDING LSTM training\n",
      "Epoch 1/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.1636 - acc: 0.4924\n",
      "Epoch 2/25\n",
      "5774/5774 [==============================] - 5s 899us/step - loss: 1.0796 - acc: 0.5488\n",
      "Epoch 3/25\n",
      "5774/5774 [==============================] - 5s 889us/step - loss: 1.0372 - acc: 0.5788\n",
      "Epoch 4/25\n",
      "5774/5774 [==============================] - 6s 1ms/step - loss: 1.0155 - acc: 0.5958\n",
      "Epoch 5/25\n",
      "5774/5774 [==============================] - 5s 811us/step - loss: 0.9946 - acc: 0.5987\n",
      "Epoch 6/25\n",
      "5774/5774 [==============================] - 5s 802us/step - loss: 0.9729 - acc: 0.6074\n",
      "Epoch 7/25\n",
      "5774/5774 [==============================] - 4s 773us/step - loss: 0.9511 - acc: 0.6190\n",
      "Epoch 8/25\n",
      "5774/5774 [==============================] - 5s 796us/step - loss: 0.9397 - acc: 0.6295\n",
      "Epoch 9/25\n",
      "5774/5774 [==============================] - 5s 806us/step - loss: 0.9187 - acc: 0.6429\n",
      "Epoch 10/25\n",
      "5774/5774 [==============================] - 5s 799us/step - loss: 0.8900 - acc: 0.6467\n",
      "Epoch 11/25\n",
      "5774/5774 [==============================] - 5s 795us/step - loss: 0.8593 - acc: 0.6597\n",
      "Epoch 12/25\n",
      "5774/5774 [==============================] - 5s 817us/step - loss: 0.8289 - acc: 0.6723\n",
      "Epoch 13/25\n",
      "5774/5774 [==============================] - 5s 798us/step - loss: 0.7936 - acc: 0.6907\n",
      "Epoch 14/25\n",
      "5774/5774 [==============================] - 5s 801us/step - loss: 0.7613 - acc: 0.7056\n",
      "Epoch 15/25\n",
      "5774/5774 [==============================] - 5s 814us/step - loss: 0.7135 - acc: 0.7229\n",
      "Epoch 16/25\n",
      "5774/5774 [==============================] - 5s 815us/step - loss: 0.6665 - acc: 0.7445\n",
      "Epoch 17/25\n",
      "5774/5774 [==============================] - 5s 811us/step - loss: 0.6235 - acc: 0.7634\n",
      "Epoch 18/25\n",
      "5774/5774 [==============================] - 5s 811us/step - loss: 0.5686 - acc: 0.7903\n",
      "Epoch 19/25\n",
      "5774/5774 [==============================] - 5s 819us/step - loss: 0.5216 - acc: 0.8083\n",
      "Epoch 20/25\n",
      "5774/5774 [==============================] - 5s 816us/step - loss: 0.4612 - acc: 0.8320\n",
      "Epoch 21/25\n",
      "5774/5774 [==============================] - 5s 822us/step - loss: 0.4303 - acc: 0.8417\n",
      "Epoch 22/25\n",
      "5774/5774 [==============================] - 5s 815us/step - loss: 0.3714 - acc: 0.8706\n",
      "Epoch 23/25\n",
      "5774/5774 [==============================] - 5s 825us/step - loss: 0.3398 - acc: 0.8807\n",
      "Epoch 24/25\n",
      "5774/5774 [==============================] - 5s 880us/step - loss: 0.2879 - acc: 0.9035\n",
      "Epoch 25/25\n",
      "5774/5774 [==============================] - 5s 863us/step - loss: 0.2565 - acc: 0.9127\n",
      "Finish RNN PRETRAIN EMBEDDING LSTM training\n",
      "1444/1444 [==============================] - 0s 341us/step\n"
     ]
    }
   ],
   "source": [
    "y_labels_pretrain_embeddings_rnn = classifcation_pretrain_embedings_rnn(tweets, train_index, test_index, labels_train,input_embeddings_path,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "H9Zhz6HmZ0GU",
    "outputId": "c52b85f5-ece8-4619-def8-d286183556bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Results SVM***\n",
      "Macro-Precision: 0.4383837005174791\n",
      "Macro-Recall: 0.4231233155582803\n",
      "Macro-F1: 0.41892876726107364\n",
      "Accuracy: 0.5533240997229917\n",
      "\n",
      "*** Results RNN_LSTM***\n",
      "Macro-Precision: 0.3449534569476766\n",
      "Macro-Recall: 0.30921803034448014\n",
      "Macro-F1: 0.2449266958839656\n",
      "Accuracy: 0.43005540166204986\n",
      "\n",
      "*** Results RNN_EMBEDINGS_LSTM***\n",
      "Macro-Precision: 0.4226924587486431\n",
      "Macro-Recall: 0.38224974868015993\n",
      "Macro-F1: 0.372017775860445\n",
      "Accuracy: 0.4930747922437673\n",
      "\n",
      "*** Results RNN_PRETRAIN_EMBEDINGS_LSTM***\n",
      "Macro-Precision: 0.4297365882135207\n",
      "Macro-Recall: 0.4253656851383887\n",
      "Macro-F1: 0.4267003418199647\n",
      "Accuracy: 0.5249307479224377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "calculate_quality_performamnce(labels_test, y_labels_svn, \"SVM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_rnn, \"RNN_LSTM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_embeddings_rnn, \"RNN_EMBEDINGS_LSTM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_pretrain_embeddings_rnn, \"RNN_PRETRAIN_EMBEDINGS_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "caepia_tutorial_codigo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
