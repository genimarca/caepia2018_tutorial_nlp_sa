{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTVbXvqsI2-F"
   },
   "source": [
    "# Cabecera\n",
    "* Importación de librerías\n",
    "* Definición de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iZkoHKkbJPoI",
    "outputId": "50036a46-03f6-4a98-9995-5d942f5c9b2a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy.random import rand as np_rand\n",
    "from numpy.random import seed as np_seed\n",
    "from numpy import array as np_array\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.svm.classes import LinearSVC\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUgbBfpiJS-Z"
   },
   "outputs": [],
   "source": [
    "TWEET_TOKENIZER = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "CLASSES = []\n",
    "EMB_SEP_CHAR = \" \"\n",
    "RE_TOKEN_USER = re.compile(r\"(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){20}(?!@))|(?<![A-Za-z0-9_!@#\\$%&*])@(([A-Za-z0-9_]){1,19})(?![A-Za-z0-9_]*@)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqobKswAI4Jn"
   },
   "source": [
    "# Lectura de Datos\n",
    "* Lectura corpus\n",
    "* Lectura embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwRPCvuZRGE-"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    '''Load the corpus into memory\n",
    "    '''\n",
    "    \n",
    "    ids = []\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    ids_append = ids.append\n",
    "    classes_append = CLASSES.append\n",
    "    labels_append = labels.append\n",
    "    tweets_append = tweets.append\n",
    "    with(open(path, 'r', encoding='utf-8')) as input_file:\n",
    "        own_split = str.split\n",
    "        own_strip = str.strip\n",
    "        input_file.readline()\n",
    "        for buffer in input_file:\n",
    "            buffer_fields = own_split(buffer, ';;;')\n",
    "            ids_append(own_strip(buffer_fields[0]))\n",
    "            label = own_strip(buffer_fields[4])\n",
    "            if(label not in CLASSES):\n",
    "                classes_append(label)\n",
    "            labels_append(CLASSES.index(label))\n",
    "            tweets_append(own_strip(buffer_fields[-1]))\n",
    "    \n",
    "    return(ids, labels, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cckyuMPVRct-"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(path, offset):\n",
    "    \"\"\"Load embeddings file.\n",
    "    \"\"\"\n",
    "    word_embeddings = [[] for i in range(offset)]\n",
    "    word_indexes = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as emb_file:\n",
    "        emb_file.readline()\n",
    "        for line in emb_file:\n",
    "            fields = line.partition(EMB_SEP_CHAR)\n",
    "            word = fields[0].strip()\n",
    "            own_strip = str.strip\n",
    "            emb_values = np_array([float(x) for x in own_strip(fields[-1]).split(EMB_SEP_CHAR)])\n",
    "            word_indexes[word] = len(word_embeddings)\n",
    "            word_embeddings.append(emb_values)\n",
    "\n",
    "    return (word_embeddings, word_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBOTRiwOSo04"
   },
   "source": [
    "# Utilidades\n",
    "* Función para tokenizar.\n",
    "* Función para generar entrada RNN\n",
    "* Función para generar entrada RNN con embeddings pre-entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3E417tbYTbjP"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize an input text\n",
    "    \n",
    "    Args:\n",
    "        text: A String with the text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        A list of Strings (tokens)\n",
    "    \"\"\"\n",
    "    text_tokenized = TWEET_TOKENIZER.tokenize(text)\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeEP60haTjte"
   },
   "outputs": [],
   "source": [
    "def fit_transform_vocabulary(corpus):\n",
    "    \"\"\"Creates the vocabulary of the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list os str (documents)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple whose first element is a dictionary word-index and the second\n",
    "        element is a list of list in which each position is the index of the \n",
    "        token in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = {}\n",
    "    corpus_indexes = []\n",
    "    corpus_indexes_append = corpus_indexes.append\n",
    "    index = 2\n",
    "    for doc in corpus:\n",
    "        doc_indexes = []\n",
    "        tokens = tokenize(doc)\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = index\n",
    "                doc_indexes.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                doc_indexes.append(vocabulary[token])\n",
    "        \n",
    "        corpus_indexes_append(doc_indexes)\n",
    "        \n",
    "    return (vocabulary, corpus_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW25G4Z4Tnlm"
   },
   "outputs": [],
   "source": [
    "def fit_transform_vocabulary_pretrain_embeddings(corpus, pre_embeddings_index):\n",
    "    \"\"\"Creates the vocabulary of the corpus.\n",
    "        Index 0: padding\n",
    "        Index 1: OOV.\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list os str (documents)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple whose first element is a dictionary word-index and the second\n",
    "        element is a list of list in which each position is the index of the \n",
    "        token in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = {}\n",
    "    corpus_indexes = []\n",
    "    corpus_indexes_append = corpus_indexes.append\n",
    "    index = 0\n",
    "    own_lowercase = str.lower\n",
    "    for doc in corpus:\n",
    "        doc_indexes = []\n",
    "        tokens = tokenize(own_lowercase(doc))\n",
    "        for token in tokens:\n",
    "            if RE_TOKEN_USER.fullmatch(token):\n",
    "                token = \"@user\"\n",
    "            if token in pre_embeddings_index:\n",
    "                index = pre_embeddings_index[token]\n",
    "                doc_indexes.append(index)\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = index\n",
    "            else:\n",
    "                index = 1\n",
    "                doc_indexes.append(index)\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = index\n",
    "        corpus_indexes_append(doc_indexes)\n",
    "        \n",
    "    return (vocabulary, corpus_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSw4mkXWTtuv"
   },
   "source": [
    "# Clasificadores\n",
    "\n",
    "\n",
    "*   SVM con TF-IDF\n",
    "*   RNN-LSTM con TF-IDF\n",
    "*   RNN-LSTM con embeddings aleatorios\n",
    "*   RNN-LSTM con embeddings pre-entrenados (FastText)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN4BHYLcUKzU"
   },
   "outputs": [],
   "source": [
    "def classification_linear_svm(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classifies using SVM as classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    \n",
    "    classifier = LinearSVC(multi_class=\"ovr\", random_state=random_state)\n",
    "    print(\"Start SVM training\")\n",
    "    classifier = classifier.fit(train_sparse_matrix_features_tfidf, labels_train)\n",
    "    print(\"Finish SVM training\")\n",
    "    y_labels = classifier.predict(test_sparse_matrix_features_tfidf)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9r-rCv1Vqvb"
   },
   "outputs": [],
   "source": [
    "def classification_tfidf_rnn(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification using a RNN with tfidf as features\n",
    "    \"\"\"\n",
    "    np_seed(random_state)\n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    train_features_tfidf = []\n",
    "    own_train_features_tfidf_append = train_features_tfidf.append\n",
    "    lengths_tweets = []\n",
    "    own_lengths_tweets_append = lengths_tweets.append\n",
    "    \n",
    "    for tweet in train_sparse_matrix_features_tfidf:\n",
    "        own_train_features_tfidf_append(tweet.data)\n",
    "        own_lengths_tweets_append(len(tweet.data))\n",
    "    \n",
    "\n",
    "    test_features_tfidf = [tweet.data for tweet in test_sparse_matrix_features_tfidf]\n",
    "    #Average length\n",
    "    max_len_input = int(np.average(lengths_tweets, 0))\n",
    "    #lstm_output_dim = int(2**np.log2(max_len_input))\n",
    "    \n",
    "    #NN model\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(LSTM(64, input_shape=(max_len_input,1)))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[0]))\n",
    "    print(\"Training features:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[-1]))\n",
    "    print(\"Training parameters:\\t{}\".format(train_sparse_matrix_features_tfidf.shape[0]*train_sparse_matrix_features_tfidf.shape[-1]))\n",
    "    print(nn_model.summary())\n",
    "\n",
    "    train_features_tfidf_pad = sequence.pad_sequences(train_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=train_sparse_matrix_features_tfidf.dtype)\n",
    "    train_features_tfidf_pad = np.expand_dims(train_features_tfidf_pad, axis=-1)\n",
    "    print(\"Start RNN LSTM training\")\n",
    "    nn_model.fit(train_features_tfidf_pad, labels_train, batch_size=32, epochs=10, verbose=1)\n",
    "    print(\"Finish RNN LSTM training\")\n",
    "    test_features_tfidf_pad = sequence.pad_sequences(test_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=test_sparse_matrix_features_tfidf.dtype)\n",
    "    test_features_tfidf_pad = np.expand_dims(test_features_tfidf_pad, axis=-1)\n",
    "    y_labels = nn_model.predict_classes(test_features_tfidf_pad, batch_size=32, verbose=1)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJoyKyT5Ykad"
   },
   "outputs": [],
   "source": [
    "def classifcation_embedings_rnn(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification with RNN and embedings (no pre-trained)\n",
    "    \"\"\"\n",
    "    np_seed(random_state)\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "\n",
    "    #Build vocabulary and corpus indexes\n",
    "    vocabulary_train, corpus_train_index = fit_transform_vocabulary(tweets_train)\n",
    "    \n",
    "    \n",
    "    max_len_input = int(np.average([len(tweet_train) for tweet_train in corpus_train_index], 0))\n",
    "    \n",
    "    corpus_test_index = []\n",
    "    own_corpus_test_index_append = corpus_test_index.append\n",
    "    for tweet_test in tweets_test:\n",
    "        tokens_test = tokenize(tweet_test)\n",
    "        own_corpus_test_index_append([vocabulary_train.get(token_test, 1) for token_test in tokens_test])\n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Embedding(len(vocabulary_train)+2, 100, input_length=max_len_input, trainable=False))\n",
    "    nn_model.add(LSTM(64))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(len(tweets_train)))\n",
    "    print(\"Training features (vocabulary):\\t{}\".format(len(vocabulary_train)))\n",
    "    print(\"Training doc x features:\\t{}\".format(len(tweets_train)*len(vocabulary_train)))\n",
    "    print(\"Training vocabulary embeddings:\\t{}\".format(len(vocabulary_train)*100))\n",
    "    print(\"Training parameters:\\t{}\".format(len(tweets_train)*len(vocabulary_train)*100))\n",
    "    print(nn_model.summary())\n",
    "    \n",
    "\n",
    "    train_features_pad = sequence.pad_sequences(corpus_train_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train_index[0][0]))\n",
    "    \n",
    "    \n",
    "    print(\"Start RNN EMBEDDING LSTM training\")\n",
    "    nn_model.fit(train_features_pad, labels_train, batch_size=32, epochs=25, verbose=1)\n",
    "    print(\"Finish RNN EMBEDDING LSTM training\")\n",
    "    test_features_pad = sequence.pad_sequences(corpus_test_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_test_index[0][0]))\n",
    "    y_labels = nn_model.predict_classes(test_features_pad, batch_size=32, verbose=1)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxwKgQoJYuP3"
   },
   "outputs": [],
   "source": [
    "def classifcation_pretrain_embedings_rnn(tweets, train_index, test_index, labels_train, embeddings_path, random_state=None):\n",
    "    \"\"\"Classification with RNN and embedings (no pre-trained)\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "\n",
    "    #Offset = 2; Padding and OOV.\n",
    "    print(\"Begin loading embeddings.\")\n",
    "    word_embeddings, word_emb_indexes = read_embeddings(embeddings_path, 2)\n",
    "    print(\"End loading embeddings.\")\n",
    "    np_seed(random_state)\n",
    "    word_embeddings[0] = 2 * 0.1 * np_rand(len(word_embeddings[2])) - 1\n",
    "    word_embeddings[1] = 2 * 0.1 * np_rand(len(word_embeddings[2])) - 1\n",
    "\n",
    "    #Build vocabulary and corpus indexes\n",
    "    vocabulary_train, corpus_train_index = fit_transform_vocabulary_pretrain_embeddings(tweets_train, word_emb_indexes)\n",
    "    \n",
    "    \n",
    "    max_len_input = int(np.average([len(tweet_train) for tweet_train in corpus_train_index], 0))\n",
    "    \n",
    "    corpus_test_index = []\n",
    "    own_corpus_test_index_append = corpus_test_index.append\n",
    "    own_lowercase = str.lower\n",
    "    for tweet_test in tweets_test:\n",
    "        tokens_test = tokenize(own_lowercase(tweet_test))\n",
    "        doc_test_index = []\n",
    "        for token_test in tokens_test:\n",
    "            if RE_TOKEN_USER.fullmatch(token_test) is not None:\n",
    "                token_test = \"@user\"\n",
    "            doc_test_index.append(word_emb_indexes.get(token_test, 1))\n",
    "        own_corpus_test_index_append(doc_test_index)\n",
    "    \n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Embedding(len(word_embeddings), len(word_embeddings[0]), weights=[np_array(word_embeddings)], input_length=max_len_input, trainable=False))\n",
    "    nn_model.add(LSTM(64))\n",
    "    nn_model.add(Dense(32, activation='tanh'))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    print(\"Summary of the model\")\n",
    "    print(\"Training samples:\\t{}\".format(len(tweets_train)))\n",
    "    print(\"Training features (vocabulary/embeddings):\\t{}\".format(len(word_embeddings)))\n",
    "    print(\"Training doc x features:\\t{}\".format(len(tweets_train)*len(word_embeddings)))\n",
    "    print(\"Training vocabulary embeddings:\\t{}\".format(len(word_embeddings)*100))\n",
    "    print(\"Training input features:\\t{}\".format(len(tweets_train)*max_len_input))\n",
    "    print(\"Training input features:\\t{}\".format(len(tweets_train)*max_len_input*100))\n",
    "    print(\"Training parameters:\\t{}\".format(len(tweets_train)*len(word_embeddings)*100))\n",
    "    print(nn_model.summary())\n",
    "    \n",
    "\n",
    "    train_features_pad = sequence.pad_sequences(corpus_train_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train_index[0][0]))\n",
    "    \n",
    "    \n",
    "    print(\"Start RNN PRETRAIN EMBEDDING LSTM training\")\n",
    "    nn_model.fit(train_features_pad, labels_train, batch_size=32, epochs=25, verbose=1)\n",
    "    print(\"Finish RNN PRETRAIN EMBEDDING LSTM training\")\n",
    "    test_features_pad = sequence.pad_sequences(corpus_test_index, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_test_index[0][0]))\n",
    "    y_labels = nn_model.predict_classes(test_features_pad, batch_size=32, verbose=1)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DITTgeFzYwy5"
   },
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMSuCFRzYy-e"
   },
   "outputs": [],
   "source": [
    "def calculate_quality_performamnce(y_labels, y_classified_labels, model_name):\n",
    "    \n",
    "    classes_index = [CLASSES.index(c) for c in CLASSES]\n",
    "    accruacy = metrics.accuracy_score(y_labels, y_classified_labels)\n",
    "    macro_precision = metrics.precision_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_recall = metrics.recall_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_f1 = metrics.f1_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    \n",
    "    print(\"\\n*** Results \" + model_name + \"***\")\n",
    "    print(\"Macro-Precision: \" + str(macro_precision))\n",
    "    print(\"Macro-Recall: \" + str(macro_recall))\n",
    "    print(\"Macro-F1: \" + str(macro_f1))\n",
    "    print(\"Accuracy: \" + str(accruacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfVqkjSKY3XA"
   },
   "source": [
    "# Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-175WWrzY9oC"
   },
   "source": [
    "Configuración semilla aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFWooq_rY5PX"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJMeepvfZFCY"
   },
   "outputs": [],
   "source": [
    "input_file_path=\"tass14_general_corpus_train.csv\"\n",
    "input_embeddings_path=\"fasttext_spanish_twitter_100d.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bta9PLPTZioe"
   },
   "source": [
    "Leer corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atNMjlXSZfOa"
   },
   "outputs": [],
   "source": [
    "ids, labels, tweets = read_corpus(input_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMpx1ZQZZmC2"
   },
   "source": [
    "Partición entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYZ-7SloZoM8"
   },
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(np.arange(len(tweets)), test_size=0.2, random_state=7)\n",
    "labels_train = [labels[tweet_index] for tweet_index in train_index]\n",
    "labels_test = [labels[tweet_index] for tweet_index in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJNpKmVeZs_N"
   },
   "source": [
    "Ejecución clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yx_C9gAeZwk-",
    "outputId": "c67d2250-7ffa-41a9-9ea2-17b97b06218a"
   },
   "outputs": [],
   "source": [
    "y_labels_svn = classification_linear_svm(tweets, train_index, test_index, labels_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "Jcph9Np0cUtp",
    "outputId": "b5cce772-ea59-4606-86d3-60aedc30eaa0"
   },
   "outputs": [],
   "source": [
    "y_labels_rnn = classification_tfidf_rnn(tweets, train_index, test_index, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "id": "CZD-fL7hcZDA",
    "outputId": "525ff7c1-133c-4a6a-e58c-220d9d2a0ca6"
   },
   "outputs": [],
   "source": [
    "y_labels_embeddings_rnn = classifcation_embedings_rnn(tweets, train_index, test_index, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1360
    },
    "colab_type": "code",
    "id": "URAG5MRbcaON",
    "outputId": "cf02d958-f792-46f1-eaa0-79b463c5c846"
   },
   "outputs": [],
   "source": [
    "y_labels_pretrain_embeddings_rnn = classifcation_pretrain_embedings_rnn(tweets, train_index, test_index, labels_train,input_embeddings_path,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "H9Zhz6HmZ0GU",
    "outputId": "c52b85f5-ece8-4619-def8-d286183556bf"
   },
   "outputs": [],
   "source": [
    "calculate_quality_performamnce(labels_test, y_labels_svn, \"SVM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_rnn, \"RNN_LSTM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_embeddings_rnn, \"RNN_EMBEDINGS_LSTM\")\n",
    "calculate_quality_performamnce(labels_test, y_labels_pretrain_embeddings_rnn, \"RNN_PRETRAIN_EMBEDINGS_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "caepia_tutorial_codigo.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
